{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains functions for downloading and saving data from the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6). Information about NEX-GDDP-CMIP6 is available at https://www.nature.com/articles/s41597-022-01393-4. To access and process this data, users will need credentials for the Fix6 Amazon S3 bucket.\n",
    "\n",
    "To begin, create a hidden.py file with the necessary S3 bucket credentials. An example template is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_AWS_ACCESS_KEY_ID'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_AWS_SECRET_ACCESS_KEY'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows you to access modules saved in other directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_environment(relative_folderpath):\n",
    "    \"\"\"\n",
    "    Configure the environment path to include the specified relative directory.\n",
    "\n",
    "    Parameters:\n",
    "    - relative_folderpath: The relative path to the directory to be added to the system path.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    absolute_folderpath = os.path.abspath(os.path.join(os.getcwd(), relative_folderpath))\n",
    "    sys.path.append(absolute_folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_environment('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "import hidden\n",
    "from nex_gddp_cmip6 import get_nex_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load saved polygons as GeoDataFrames, use the polygons to clip an Xarray dataset, split the Xarray dataset into train, validate, and test datasets, and save the result as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_polygons(folderpath, filename):\n",
    "    \"\"\"\n",
    "    Load a GeoDataFrame from the specified processed directory.\n",
    "\n",
    "    Parameters:\n",
    "    - folderpath (str): The path to the main folder containing the processed subfolder.\n",
    "    - filename (str): The name of the file (with extension) to load from the processed directory.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: A GeoDataFrame loaded from the specified file in the processed subfolder.\n",
    "    \"\"\"\n",
    "    # Create a Path object for folderpath to ensure correct path manipulation\n",
    "    folder = Path(folderpath)\n",
    "\n",
    "    # Construct the file path for the processed version of the file\n",
    "    filepath = folder / 'processed' / filename\n",
    "    \n",
    "    # Load and return the GeoDataFrame\n",
    "    return gpd.read_file(str(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_dataset(dataset, geodataframe):\n",
    "    \"\"\"\n",
    "    Clip a dataset by a GeoDataFrame's boundaries.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset to be clipped.\n",
    "    - geodataframe: A GeoDataFrame that defines the region to clip.\n",
    "\n",
    "    Returns:\n",
    "    - The clipped dataset.\n",
    "    \"\"\"\n",
    "    ds = dataset.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "    ds = ds.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    gdf = geodataframe.to_crs(ds.rio.crs)\n",
    "    \n",
    "    return ds.rio.clip(gdf.geometry.apply(mapping), gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, time_splits):\n",
    "    \"\"\"\n",
    "    Split a dataset into multiple datasets based on specified time splits.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The xarray.Dataset to be split.\n",
    "    - time_splits: A list of numpy.datetime64 objects indicating the split points.\n",
    "\n",
    "    Returns:\n",
    "    - A list of xarray.Dataset objects representing the datasets split according to the time points.\n",
    "      The length of the returned list is one more than the number of splits, as it includes the ranges\n",
    "      before the first split, between each pair of splits, and after the last split.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    previous_time = None\n",
    "    \n",
    "    for current_time in time_splits:\n",
    "        if previous_time is None:\n",
    "            # Before the first split point\n",
    "            ds_split = dataset.sel(time=(dataset.time < current_time))\n",
    "        else:\n",
    "            # Between the current and previous split points\n",
    "            ds_split = dataset.sel(time=(dataset.time >= previous_time) & (dataset.time < current_time))\n",
    "        datasets.append(ds_split)\n",
    "        previous_time = current_time\n",
    "        \n",
    "    # After the last split point\n",
    "    datasets.append(dataset.sel(time=(dataset.time >= time_splits[-1])))\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, folderpath, filename):\n",
    "    \"\"\"\n",
    "    Save the dataset to a netCDF file. \n",
    "    The netCDF file is saved in a raw subdirectory within the specified folder path.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: The xarray.Dataset to be saved.\n",
    "    - folderpath: A string specifying the directory path where the netCDF file will be saved.\n",
    "    - filename: The name of the netCDF file to save without an extension.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a Path object for folderpath to ensure correct path manipulation\n",
    "    folder = Path(folderpath)\n",
    "    \n",
    "    # Construct the file path for the processed version of the file\n",
    "    filepath = folder / 'raw' / (filename + '.nc')\n",
    "    \n",
    "    dataset.to_netcdf(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from S3\n",
    "ds = get_nex_dataset(['tasmin'], ['projection'])\n",
    "\n",
    "# Load GeoDataFrame\n",
    "gdf = load_polygons('../data', 'gdf_easternmountain_polygons')\n",
    "\n",
    "# Clip dataset\n",
    "ds_clipped = clip_dataset(ds, gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that according to Ibrahim N. Mohammed (2024), the \"NEX-GDDP-CMIP6 climate projections is downscaled at a spatial resolution of 0.25 degrees x 0.25 degrees (approximately 25 km x 25 km).\" https://imohamme.github.io/NASAaccess/articles/NEXGDDP-CMIP6.html. In contrast, the OikoLab ERA5 dataset has a spatial resolution of approximately 28 km x 28 km. https://docs.oikolab.com/. Interpolation is required to merge the NEX-GDDP-CMIP6 and ERA5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods\n",
    "dt_train = np.datetime64('2022', 'ns')\n",
    "dt_validate = np.datetime64('2023', 'ns')\n",
    "dt_test = np.datetime64('2024', 'ns')\n",
    "dt_project = np.datetime64('2051', 'ns')\n",
    "\n",
    "dts = [dt_train, dt_validate, dt_test, dt_project]\n",
    "\n",
    "# Split dataset into train, validate, and test datasets\n",
    "ds_train, ds_validate, ds_test, ds_project, _ = split_dataset(ds_clipped, dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dataset(ds_train, '../data', 'CMIP6_train_easternmountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dataset(ds_validate, '../data', 'CMIP6_validate_easternmountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dataset(ds_test, '../data', 'CMIP6_test_easternmountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dataset(ds_project, '../data', 'CMIP6_project_easternmountain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
